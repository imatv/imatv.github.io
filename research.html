<!DOCTYPE html>
<html>
    <head>
        <title>
            Research
        </title>
        <link rel="stylesheet" type="text/css" href="static/css/style.css" />
		<meta charset="UTF-8" />
        <link rel='shortcut icon' href='static/images/favicon.ico' type='image/x-icon'/ >
    </head>
    
    <div>
        <div id="navbar-left">
            <a href="index.html"><img class="site-logo-notmain" src="static/images/site-logo.png" alt="Site Logo"></a>
            <br>
        </div>
        <div id="navbar-right">
            <br>
            <a href="about.html">
                <img class="profile-icon" src="static/images/icons/user.svg" alt="Profile Icon">
            </a>
            &nbsp;&nbsp;&nbsp;&nbsp;
	    <a href="vrdev.html">
                <img class="mainIcons-navbar" src="static/images/icons/virtualreality.svg" alt="VR Icon">
            </a>
            &nbsp;&nbsp;&nbsp;&nbsp;
	    <img class="mainIcons-navbar" src="static/images/icons/brainColored.svg" alt="Brain Icon">
            &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="hacking.html">
                <img class="mainIcons-navbar" src="static/images/icons/keyboard.svg" alt="Keyboard Icon">
            </a>
            &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="archery.html">
                <img class="mainIcons-navbar" src="static/images/icons/archery.svg" alt="Archery Icon">
            </a>
            &nbsp;&nbsp;&nbsp;&nbsp;
        </div>
    </div>
    <br><br><br><br><br><br>
    
    <header>
        <a href="http://rissmanlab.psych.ucla.edu/rissmanlab/Home.html" target="_blank">
            <img class="rissmanLabLogo" src="static/images/research/RissmanLabLogo.jpg" alt="Rissman Logo">
        </a>
    </header>
    
    <body class="pages">
        <br>
        <p> <a href="http://rissmanlab.psych.ucla.edu/rissmanlab/Home.html" target="_blank">Rissman Memory Lab </a> </p>
        
        <div id="wrapper">
            Under the supervision of Dr. Jesse Rissman, I work as a research assistant in a cognitive 
            neuroscience lab at UCLA.  Listed below are the projects that I am a part of:<br>
            
            <ul>
                <li> <em>Neural Correlates of Behavioral Measures via the Human Connectome Project</em>
                <br class="smaller"><br class="smaller">
                    <ul> 
                        <li class="smaller">
                            With graduate student, <a href="http://philoneuro.com/" target="_blank"> Nicco Reggente</a>, we used 
                            functional magnetic resonance imaging (fMRI) and diffusion tensor imaging (DTI) data from the 
                            <a href="http://www.humanconnectome.org/" target="_blank">Human Connectome Project (HCP)</a> to account for individual differences  in fluid intelligence, memory strength, and reward. 
                        <br class="smaller"><br class="smaller">
                        <li class="smaller">
                            I wrote Matlab and shell scripts to train predictive models using many subjects' data and assess the relationships to 
                            their behavioral scores on the <a href="https://en.wikipedia.org/wiki/Raven's_Progressive_Matrices" target="_blank">Penn Progressive Matrices</a> test and other various 
                            <a href="http://www.nihtoolbox.org/Pages/default.aspx" target="blank">NIH Toolbox</a> tasks.
			<br class="smaller"><br class="smaller">
			<li class="smaller">
                            I based my CS 170A final project on this work. The final writeup can be found <a href="classes/CS170ReportBC.pdf" target="_blank">here</a>, as well as the <a href="static/files/Posters/NIQ_PURC_2016_Final.pdf" target="_blank">poster</a>. 
			    You can also view this project's <a href="https://github.com/imatv/rissmanLab-scripts" target="_blank">code</a>. 
            		    <br><br>
                    </ul>
                <li> <em> Avatar Learning in Virtual Environments </em>
                <br class="smaller"><br class="smaller">
                    <ul> 
                        <li class="smaller">
                            As a part of the <a href="http://www.darpa.mil/" target="_blank">DARPA</a>-sponsored <a href="https://jessoe.wordpress.com/research/alive/" target="_blank">ALIVE</a> project, working under 
                            graduate students, <a href="https://jessoe.wordpress.com/" target="_blank">Joey Essoe</a> and 
                            <a href="http://philoneuro.com/" target="_blank">Nicco Reggente</a>, we are investigating the cognitive and 
                            neural mechanisms of learning and memory that occurs within virtual reality (VR) and their implications for future training
                            and educational purposes.
                        <br class="smaller"><br class="smaller">
                        <li class="smaller">
                            <a href="http://www.bmap.ucla.edu/services/safety_training/mri_safety/" target="_blank">MRI safety</a> certified
                            and <a href="https://www.citiprogram.org/" target="_blank">CITI</a>-trained 
                            (basic, social/behavioral, HIPAA), I assist in the process of running and scoring participants, as well as maintaining data quality assurance. <br>                    </ul>
            </ul>
        </div>
        
        <br><br><br>
        
        <div id="border">
        </div>
        
        <br><br>
	
	<a href="https://zililab.psych.ucla.edu/" target="_blank">
            <img class="ziliLabLogo" src="static/images/research/ZiliLabLogo.png" alt="Zili Logo">
        </a>
	    
	<br><br>
        <p> <a href="https://zililab.psych.ucla.edu/" target="_blank">Zili Perceptual Processing Lab</a> </p>
	
	<div id="wrapper">
            Under the supervision of Dr. Zili Liu, I work as a research assistant in a computational perceptual processing 
            lab at UCLA.  Listed below are the projects that I am a part of:<br>
            
            <ul>
                <li> <em>Computational Motion Processing &amp; Learning</em>
                <br class="smaller"><br class="smaller">
                    <ul> 
                        <li class="smaller">
                            Worked with <a href="https://zililab.psych.ucla.edu/lab-members/yang-xing-mac/" target="_blank">Yang (Mac) Xing</a> 
			    on writing/modifying Matlab scripts for analyses of MT-V1 task-state fMRI data in an attempt to better understand 
			    motion processing and perceptual learning using SVM techniques. <br><br>
                    </ul>
                <li> <em> Environmental Vertical Illusion in Virtual Reality </em>
                <br class="smaller"><br class="smaller">
                    <ul> 
                        <li class="smaller">
                            Working with <a href="https://zililab.psych.ucla.edu/lab-members/chela-willey/" target="_blank">Chéla Willey</a> on 
			    investigating the oculovestibular perception of virtually tilted rooms. Tilted/skewed contexts affect human perception 
			    of their subjective vertical, resulting in environmental illusions.
                        <br class="smaller"><br class="smaller">
                        <li class="smaller">
                            I created the experimental virtual environment and its functionalities in Unity. <br>
			    You can also view this project's <a href="https://github.com/imatv/ziliLab-VR-OVI" target="_blank">code</a>. 
                    </ul>
            </ul>
        </div>
	    
	<br><br><br>
        
        <div id="border">
        </div>
        
        <br><br>
        <p> Poster Presentations </p>
        
        <div id="wrapper4">
            <ul>
		<li> <!--<a href="static/files/Posters/_.pdf" target="_blank">-->
                        Long-Term Retention of Vocabulary in Two Phonetically Similar Foreign Languages Is Aided When Learning Occurs in Highly Distinctive Virtual Reality Environments
                     <!--</a>-->
                <br class="smaller"><br class="smaller">
                    <ul> 
                    <li class="smaller">
                            <a href="https://www.cogneurosociety.org/" target="_blank">CNS</a> 2017 - San Francisco, CA
                            <br><br class="smaller">
                        <li class="smaller">
                            Joey Ka-Yee Essoe, Niccolo Reggente, Younji Hera Baek, Ai Aileen Ohno, Priyanka Mehta, <b>Alvin Vuong</b>, Jesse Rissman
                            <br><br class="smaller">
                        <li class="smaller">
                            <p class="smaller-descript">The environmental context in which a memory is encoded can impact its later accessibility by virtue of tagging the memory with unique retrieval cues. We examined whether distinctive virtual environments (VEs) could be used as a means to provide contextual support during the learning of two sets of easily confusable stimuli. Specifically, we taught participants the translations of 50 English words in two pre-experimentally unfamiliar languages: 10 were learned only in Swahili, 10 only in Chinyanja, and 30 in both languages. Participants in the Dual Context group learned each language in a different VE, whereas participants in the Single Context group learned both languages in the same VE. On Day 2, after the fourth VE learning session, participants’ ability to recall the Swahili and Chinyanja translations of the English words was tested outside of the VEs. One week later (Day 8), participants were reached by telephone and performed a surprise recall test assessing their long-term retention of the foreign words. Our results revealed that while the Single and Dual Context groups showed comparable recall performance when tested on Day 2, the Dual Context group exhibited significantly reduced forgetting when tested on Day 8. This finding showcases how distinctive learning contexts can protect newly acquired memories from succumbing to excessive interference and promote long-term retention. An additional fMRI dataset collected from a separate group of Dual Context participants during Day 2 cued recall should provide further insights into the mechanisms that underlie their memory advantage.</p>
                            <br><br>
                    </ul>
                <li> <a href="static/files/Posters/NIQ_PURC_2016_Final.pdf" target="_blank">
                        Neural Correlates of Fluid Intelligence via Functional and Structural Network Connectivity Measures
                     </a>
                <br class="smaller"><br class="smaller">
                    <ul> 
                    <li class="smaller">
                            <a href="https://purc.psych.ucla.edu/" target="_blank">PURC</a> 2016 - University of California, Los Angeles
                            <br><br class="smaller">
                        <li class="smaller">
                            <b>Alvin Vuong</b>, Nicco Reggente, Jesse Rissman
                            <br><br class="smaller">
                        <li class="smaller">
                            <p class="smaller-descript">Connectivity across regions in the brain can be characterized as either functional (correlated fluctuations in activity as measured by resting-state fMRI data) or structural (white matter pathways as measured by diffusion MRI data). Emerging studies suggest that the connections across brain regions that make up distinct cognitive networks can partially explain individual differences in behavioral traits. Some theorize that a reliable benchmark of intelligence is the ability to identify subtle patterns across distantly related ideas. The Raven's Progressive Matrices (RPM), a pattern completion task, is one widely used measure of general fluid intelligence. Here, we use a combination of functional and structural connectivity metrics derived from a large MRI dataset [n=127] to examine the relationship between neural connectivity and RPM scores. We used a Support Vector Regression cross-validation procedure to assess the degree to which we could predict a subject's intelligence based on these connectivity values. We were able to account for 14% of the variance in individuals' intelligence scores when using specific combinations of functional and structural connectivity values.</p>
                            <br><br>
                    </ul>
                <li> <a href="static/files/Posters/AA2_SUPC_2016_Final.pdf" target="_blank">
                        Does Presence/Immersion Confer an Advantage in Learning in Virtual Reality?
                     </a>
                <br class="smaller"><br class="smaller">
                    <ul>
                        <li class="smaller">
                            <a href="http://www.stanfordconference.org/" target="_blank">SUPC</a> 2016 - Stanford University<br>
                            <a href="http://urweek.ugresearch.ucla.edu/" target="_blank">URW</a> 2016 - University of California, Los Angeles<br>
                            URW Outstanding Poster Award
                            <br><br class="smaller">
                        <li class="smaller">
                            Ai Ohno, Hera Younji Baek, Priyanka Mehta, Jacob Yu Villa, Gabriel Hughes, Ru Ekanayake, Alana Sanchez-Prak, <b>Alvin Vuong</b>, Hugo Shiboski, Nicco Reggente, Joey Ka-Yee Essoe, Jesse Rissman
                            <br><br class="smaller">
                        <li class="smaller">
                            <p class="smaller-descript">We tested the effects of immersion (the sense of inhabiting the virtual world) and presence (the sense of embodying one's avatar) on long-term retention of information learned in virtual environments (VE). While VE are increasingly utilized in pedagogy, there is mixed evidence on the effects of presence/immersion. Some reported that increased presence/immersion resulted in better recall, while others reported the opposite. Most past research examined memory visual-spatially (testing recall of visual information about the VE itself), thus both memory and presence/immersion measures were based on the VE itself, making it difficult to disentangle the relationship between the two. We studied the effects of presence/immersion on verbal memory, thereby isolating the memory task from the VE upon which presence/immersion were based. Participants learned 40 Swahili and 40 Chinyanja vocabulary words over the course of four learning sessions in a richly featured VE. They were tested (cued by English translations) before each exposure, and later tested again outside the VE on Day 2 and Day 8. We found that individuals who reported higher levels of immersion showed significantly greater recall on Day 8 (M=.42, SD=.08), as compared to those reported low immersion (M=.17, SD=.06). However, there were no effects of presence. These results demonstrate that increased immersion, but not presence, during VE learning might facilitate long-term retention.</p>
                    </ul>
            </ul>
        </div>
	    
	<br><br><br>
        
        <div id="border">
        </div>
        
        <br><br>
        <p> Linguistics Class Research </p>
        
        <div id="wrapper">
            Some cool class research projects that I've conducted in my undergrad Linguistics courses:<br>
            
            <ul>
                <li> <em>Ling 120A: Phonology</em>
                <br class="smaller"><br class="smaller">
                    <ul> 
                        <li class="smaller">
                            Worked on a <a href="classes/Ling120ATermPaper.pdf" target="_blank">project</a> 
			    analyzing the influences of Cantonese tone changes and reduplication rules on Southern Vietnamese and vice versa of a bilingual 
			    speaker, my dad, who was forced to relocate many, many times as a refugee during the Vietnam War. This was a a more in-depth follow-up study after  
			    my Ling 103 project (described below). The main phonological phenomena I examined this time were: tonal variance and reduplication across languages, 
			    language-specific phonemic inventories, tonal harmony, tone sandhi, and phonetic rule boundaries.
			    <br><br>
                    </ul>
                <li> <em>Ling 103: Intro to Phonetics</em>
                <br class="smaller"><br class="smaller">
                    <ul> 
                        <li class="smaller">
			    Worked on a <a href="classes/Ling103TermPaper.pdf" target="_blank">project</a> 
			    analyzing the phonetics of Southern Vietnamese, again as it applies to my dad. Mainly, this project served as a throrough introduction to the 
			    many tools and methodologies that phonetics research requires: Praat, Audacity, various recording devices, background noise, 
			    elicitation scripts, phonemic and phonetic transcriptions, consonant and vowel charts, etc.
                    </ul>
            </ul>
        </div>
	    
    </body>
    
	<br><br>
    
    <div id="footer">
        <a href="https://www.facebook.com/im4tv" target="_blank">
            <img class="footer-icons" src="static/images/icons/fb.svg" alt="Facebook Icon">
        </a>
        <a href="https://www.linkedin.com/in/imatv" target="_blank">
            <img class="footer-icons" src="static/images/icons/linkedin.svg" alt="LinkedIn Icon">
        </a>
        <a href="tel:925-470-7297">
            <img class="footer-icons" src="static/images/icons/smartphone.svg" alt="Phone Icon">
        </a>
        <a href="mailto:alvin.t.vuong@ucla.edu">
            <img class="footer-icons" src="static/images/icons/email.svg" alt="Mail Icon">
        </a>
        <a href="static/files/AlvinVuong-CS-resume.pdf" target="_blank">
            <img class="footer-icons" src="static/images/icons/cv.svg" alt="CV Icon">
        </a>
    </div>
    <p class="copyright">&copy; 2017 Alvin Vuong &nbsp;</p>
    
<!--    
    <div>Icons made by <a href="http://www.flaticon.com/authors/freepik" title="Freepik">Freepik</a>, <a href="http://www.flaticon.com/authors/anas-ramadan" title="Anas Ramadan">Anas Ramadan</a>, <a href="http://www.flaticon.com/authors/simpleicon" title="SimpleIcon">SimpleIcon</a> from <a href="http://www.flaticon.com" title="Flaticon">www.flaticon.com</a> is licensed by <a href="http://creativecommons.org/licenses/by/3.0/" title="Creative Commons BY 3.0">CC BY 3.0</a></div>
-->
    
</html>
